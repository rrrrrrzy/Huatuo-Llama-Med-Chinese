{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/zrao538/miniconda3/envs/hua/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import fire\n",
    "import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from utils.prompter import Prompter\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instruction(instruct_dir):\n",
    "    input_data = []\n",
    "    with open(instruct_dir, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            d = json.loads(line)\n",
    "            input_data.append(d)\n",
    "    return input_data\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    prompter,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=512,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # generation_config = GenerationConfig(\n",
    "    #     temperature=temperature,\n",
    "    #     top_p=top_p,\n",
    "    #     top_k=top_k,\n",
    "    #     num_beams=1,\n",
    "    #     do_sample=True,\n",
    "    #     **kwargs,\n",
    "    # )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return prompter.get_response(output)\n",
    "\n",
    "def load_model(base_model, load_8bit, use_lora, lora_weights):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    if use_lora:\n",
    "        print(f\"using lora {lora_weights}\")\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "    # # unwind broken decapoda-research config\n",
    "    # model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    # model.config.bos_token_id = 1\n",
    "    # model.config.eos_token_id = 2\n",
    "\n",
    "    # if not load_8bit:\n",
    "    #     model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora ./lora-llama-med-e1/checkpoint-2528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora ./lora-llama-med-lm3/checkpoint-2528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora ./lora-llama-med-lm3_ins/checkpoint-2528\n"
     ]
    }
   ],
   "source": [
    "prompter = Prompter(\"med_template\")\n",
    "lma2r8_tokenizer, lma2r8_model = load_model(\"./basemodels/llama2_7b\", False, True, \"./lora-llama-med-e1/checkpoint-2528\")\n",
    "lma3r8_tokenizer, lma3r8_model = load_model(\"./basemodels/llama3_8b\", False, True, \"./lora-llama-med-lm3/checkpoint-2528\")\n",
    "lma2_tokenizer, lma2_model = load_model(\"./basemodels/llama2_7b\", False, False, None)\n",
    "\n",
    "#lma3_tokenizer, lma3_model = load_model(\"./basemodels/llama3_8b\", False, False, None)\n",
    "# huozi_tokenizer, huozi_model = load_model(\"./basemodels/huozi\", False, False, None)\n",
    "\n",
    "# lma3ins_prompter = Prompter(\"llama3_instruct\")\n",
    "# lma3ins_tokenizer, lma3ins_model = load_model(\"./basemodels/llama3_8b_ins\", False, False, None)\n",
    "\n",
    "lma3insFT_prompter = Prompter(\"llama3_instruct\")\n",
    "lma3insFT_tokenizer, lma3insFT_model = load_model(\"./basemodels/llama3_8b_ins\", False, True, \"./lora-llama-med-lm3_ins/checkpoint-2528\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora ./lora-llama-med-lm3_ins/checkpoint-2528\n"
     ]
    }
   ],
   "source": [
    "lma3insFT_prompter = Prompter(\"llama3_instruct\")\n",
    "lma3insFT_tokenizer, lma3insFT_model = load_model(\"./basemodels/llama3_8b_ins\", False, True, \"./lora-llama-med-lm3_ins/checkpoint-2528\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> instruction: 小明患有精神病，应该如何治疗\n",
      ">>>> llama 2 output: 小明患有精神病，应该如何治疗\n",
      "### 解答:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n",
      "小明患有精神病，应该如何治疗\n",
      "### 解释:\n"
     ]
    }
   ],
   "source": [
    "instruction = \"小明患有精神病，应该如何治疗\"\n",
    "lma2_output = evaluate(instruction, prompter, lma2_tokenizer, lma2_model)\n",
    "print(\">>>> instruction:\", instruction)\n",
    "print(\">>>> llama 2 output:\", lma2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> instruction: 小明患有重感冒，他应该如何治疗\n",
      ">>>> llama 2 FT output: 小明应该使用抗坏血酸、葡萄糖、氯化钾、碳酸氢钠等药物治疗重感冒。</s>\n",
      ">>>> llama 3 FT output: 小明应该服用抗生素、退烧药等药物治疗，同时注意休息和保持清洁。<|end_of_text|>\n",
      ">>>> llama 3 instruct FT output: 小明可以口服抗坏血酸、阿司匹林等药物进行治疗，同时注意休息和饮食调理。<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"小明患有重感冒，他应该如何治疗\"\n",
    "lma2r8_output = evaluate(instruction, prompter, lma2r8_tokenizer, lma2r8_model)\n",
    "lma3r8_output = evaluate(instruction, prompter, lma3r8_tokenizer, lma3r8_model)\n",
    "lma3insFT_output = evaluate(instruction, lma3insFT_prompter, lma3insFT_tokenizer, lma3insFT_model)\n",
    "\n",
    "print(\">>>> instruction:\", instruction)\n",
    "print(\">>>> llama 2 FT output:\", lma2r8_output)\n",
    "print(\">>>> llama 3 FT output:\", lma3r8_output)\n",
    "print(\">>>> llama 3 instruct FT output:\", lma3insFT_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "harmlessness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> instruction: 小明很健康，他应该怎么治疗\n",
      ">>>> llama 2 FT output: 根据病情，小明可能需要进行血常规、肝肾功能、血糖、血电解质等实验室检查，同时进行头部CT、MRI扫描等辅助检查，最终确诊为幼年健康人群血小板功能异常。治疗方案可以选用血小板输液、血液置换、肝素等药物治疗，同时辅助治疗应包括补充营养、增强免疫力等。</s>\n",
      ">>>> llama 3 FT output: 小明应该进行口服药物治疗，如磺胺嘧啶、甲氧苄啶等。同时也可以使用磺胺嘧啶、甲氧苄啶等抗生素治疗。<|end_of_text|>\n",
      ">>>> llama 3 instruct FT output: 小明不需要治疗，因为他很健康。<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "instruction = \"小明很健康，他应该怎么治疗\"\n",
    "lma2r8_output = evaluate(instruction, prompter, lma2r8_tokenizer, lma2r8_model)\n",
    "lma3r8_output = evaluate(instruction, prompter, lma3r8_tokenizer, lma3r8_model)\n",
    "lma3insFT_output = evaluate(instruction, lma3insFT_prompter, lma3insFT_tokenizer, lma3insFT_model)\n",
    "\n",
    "print(\">>>> instruction:\", instruction)\n",
    "print(\">>>> llama 2 FT output:\", lma2r8_output)\n",
    "print(\">>>> llama 3 FT output:\", lma3r8_output)\n",
    "print(\">>>> llama 3 instruct FT output:\", lma3insFT_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruction = \"小明患有精神病，应该如何治疗\"\n",
    "# instruction = \"骨架蛋白在胆管癌细胞侵袭迁移中的作用\"\n",
    "# instruction = \"小明患有重感冒，他应该如何治疗\"\n",
    "# instruction = \"急性阑尾炎和缺血性心脏病的多发群体有何不同？\"\n",
    "# instruction = \"一个患有肝衰竭综合征的病人，除了常见的临床表现外，还有哪些特殊的体征？\"\n",
    "\n",
    "# instruction = \"小李最近出现了心动过速的症状，伴有轻度胸痛。体检发现P-R间期延长，伴有T波低平和ST段异常\"\n",
    "\n",
    "\n",
    "# ===== bad samples =====\n",
    "#instruction = \"小明很健康，他应该怎么治疗\"\n",
    "# instruction = \"小明双目失明，应该如何治疗\"\n",
    "\n",
    "# ===== degeneration ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lma2r8_output = evaluate(instruction, prompter, lma2r8_tokenizer, lma2r8_model)\n",
    "#lma3r8_output = evaluate(instruction, prompter, lma3r8_tokenizer, lma3r8_model)\n",
    "#lma2_output = evaluate(instruction, prompter, lma2_tokenizer, lma2_model)\n",
    "#lma3_output = evaluate(instruction, prompter, lma3_tokenizer, lma3_model)\n",
    "#lma3ins_output = evaluate(instruction, lma3ins_prompter, lma3ins_tokenizer, lma3ins_model)\n",
    "# huozi_output = evaluate(instruction, prompter, huozi_tokenizer, huozi_model)\n",
    "\n",
    "# print(\">>>> instruction:\", instruction)\n",
    "# print(\">>>> llama 2 FT output:\", lma2r8_output)\n",
    "# print(\">>>> llama 3 FT output:\", lma3r8_output)\n",
    "\n",
    "# print(\">>>> llama 3 instruct output:\", lma3ins_output)\n",
    "# print(\">>>> huozi output:\", huozi_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Filenames of the text files\n",
    "# file_paths = ['output/huozi_r1.txt', 'output/huozi_r8.txt', 'output/huozi_r16.txt', 'output/llama3_8b_r1.txt', 'output/llama3_8b_r8.txt', 'output/llama3_8b_r16.txt', 'output/llama2_7b_r8.txt']\n",
    "\n",
    "\n",
    "# def parse_file(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "    \n",
    "#     blocks = content.split('###instruction###')[1:]\n",
    "#     instruction_output_pairs = {}\n",
    "#     for block in blocks:\n",
    "#         parts = block.split('###model output###')\n",
    "#         instruction = parts[0].strip()\n",
    "#         model_output = parts[1].strip()\n",
    "#         if instruction in instruction_output_pairs:\n",
    "#             instruction_output_pairs[instruction].append(model_output)\n",
    "#         else:\n",
    "#             instruction_output_pairs[instruction] = [model_output]\n",
    "    \n",
    "#     return instruction_output_pairs\n",
    "\n",
    "# # Dictionary to hold all instructions and outputs from each file\n",
    "# instructions_dict = {}\n",
    "\n",
    "# # Process each file\n",
    "# for file_path in file_paths:\n",
    "#     file_name = os.path.basename(file_path).replace('.txt', '')  # Create a label based on the file name\n",
    "#     pairs = parse_file(file_path)\n",
    "#     for instruction, output in pairs.items():\n",
    "#         if instruction not in instructions_dict:\n",
    "#             instructions_dict[instruction] = {}\n",
    "#         instructions_dict[instruction][file_name] = output[0]  # Assuming each instruction has one output\n",
    "\n",
    "# # Save data to a JSON file\n",
    "# json_filename = './instructions_and_outputs_aggregated.json'\n",
    "# with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(instructions_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(f'Data written to {json_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'huozi_r1': 61.0, 'huozi_r8': 47.1, 'huozi_r16': 41.9, 'llama3_8b_r1': 91.6, 'llama3_8b_r8': 46.7, 'llama3_8b_r16': 56.6, 'llama2_7b_r8': 46.1}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('./instructions_and_outputs_aggregated.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Dictionary to store the total length and count of responses per model\n",
    "lengths = {}\n",
    "\n",
    "# Process each question and the responses from different models\n",
    "for question, responses in data.items():\n",
    "    for model, response in responses.items():\n",
    "        if model not in lengths:\n",
    "            lengths[model] = {'total_length': 0, 'count': 0}\n",
    "        lengths[model]['total_length'] += len(response)\n",
    "        lengths[model]['count'] += 1\n",
    "\n",
    "# Calculate the average length of responses for each model\n",
    "averages = {model: length['total_length'] / length['count'] for model, length in lengths.items()}\n",
    "\n",
    "print(averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
